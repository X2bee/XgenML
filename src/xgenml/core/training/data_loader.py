# /src/xgenml/core/training/data_loader.py
import time
import json
import numpy as np
import pandas as pd
from typing import Tuple, List, Optional, Dict, Any
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from ..data_service import HFDataService, MLflowDataService
from ...utils.logger_config import setup_logger

logger = setup_logger(__name__)


class DataLoader:
    """
    ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ë‹´ë‹¹ í´ë˜ìŠ¤
    íƒœìŠ¤í¬ë³„ ë°ì´í„° ì¤€ë¹„ ë¡œì§ ë¶„ë¦¬
    """
    
    def __init__(self, task: str, task_config: Optional[Dict[str, Any]] = None):
        """
        Args:
            task: íƒœìŠ¤í¬ íƒ€ì… (classification, regression, timeseries, anomaly_detection, clustering)
            task_config: íƒœìŠ¤í¬ë³„ ì„¤ì •
                - timeseries: {"lookback_window": 10, "forecast_horizon": 1, "time_column": "date"}
                - anomaly_detection: {"contamination": 0.1}
                - clustering: {"n_clusters": 3}
        """
        self.task = task
        self.task_config = task_config or {}
        self.hf_service = HFDataService()
        self.mlflow_service = MLflowDataService()
        
        # ë¼ë²¨ ì¸ì½”ë”© ê´€ë ¨
        self.label_encoder: Optional[LabelEncoder] = None
        self.label_mapping: Optional[Dict] = None
        self.original_classes: Optional[List] = None
    
    def load_data(
        self,
        use_mlflow_dataset: bool = False,
        mlflow_run_id: Optional[str] = None,
        mlflow_artifact_path: str = "dataset",
        hf_repo: Optional[str] = None,
        hf_filename: Optional[str] = None,
        hf_revision: Optional[str] = None,
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """ë°ì´í„° ë¡œë“œ"""
        logger.info(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        load_start_time = time.time()
        
        if use_mlflow_dataset:
            df, data_source_info = self._load_from_mlflow(
                mlflow_run_id, mlflow_artifact_path
            )
        else:
            df, data_source_info = self._load_from_huggingface(
                hf_repo, hf_filename, hf_revision
            )
        
        load_duration = time.time() - load_start_time
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({load_duration:.2f}ì´ˆ)")
        self._log_data_info(df)
        
        return df, data_source_info
    
    def _load_from_mlflow(
        self, run_id: str, artifact_path: str
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """MLflowì—ì„œ ë°ì´í„° ë¡œë“œ"""
        if not run_id:
            raise ValueError("MLflow ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ mlflow_run_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        logger.info(f"MLflow ë°ì´í„°ì…‹ ì‚¬ìš©: run_id={run_id}")
        dataset_info = self.mlflow_service.get_dataset_info(run_id)
        logger.info(f"ë°ì´í„°ì…‹ ì •ë³´: {dataset_info.get('tags', {}).get('dataset_name', 'Unknown')}")
        
        df = self.mlflow_service.load_dataset_from_run(run_id, artifact_path)
        
        data_source_info = {
            "source_type": "mlflow",
            "mlflow_run_id": run_id,
            "mlflow_artifact_path": artifact_path,
            "dataset_metrics": dataset_info.get('metrics', {}),
        }
        
        return df, data_source_info
    
    def _load_from_huggingface(
        self, repo: str, filename: str, revision: Optional[str]
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """HuggingFaceì—ì„œ ë°ì´í„° ë¡œë“œ"""
        if not repo or not filename:
            raise ValueError("HuggingFace ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ hf_repoì™€ hf_filenameì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        logger.info(f"HuggingFace ë°ì´í„°ì…‹ ì‚¬ìš©: {repo}/{filename}")
        df = self.hf_service.load_dataset_data(repo, filename, revision)
        
        data_source_info = {
            "source_type": "huggingface",
            "hf_repo": repo,
            "hf_filename": filename,
            "hf_revision": revision or "default",
        }
        
        return df, data_source_info
    
    def _log_data_info(self, df: pd.DataFrame):
        """ë°ì´í„° ì •ë³´ ë¡œê¹…"""
        logger.info(f"ë°ì´í„°ì…‹ í˜•íƒœ: {df.shape}")
        logger.info(f"ì»¬ëŸ¼: {df.columns.tolist()}")
        logger.info(f"ë°ì´í„° íƒ€ì…:\n{df.dtypes}")
        
        null_counts = df.isnull().sum()
        if null_counts.sum() > 0:
            logger.warning(f"ê²°ì¸¡ì¹˜ ë°œê²¬:\n{null_counts[null_counts > 0]}")
        else:
            logger.info("ê²°ì¸¡ì¹˜ ì—†ìŒ")
    
    def prepare_features(
        self,
        df: pd.DataFrame,
        target_column: Optional[str] = None,
        feature_columns: Optional[List[str]] = None,
    ) -> Tuple[Any, Any, List[str], Dict[str, Any]]:
        """
        íƒœìŠ¤í¬ë³„ í”¼ì²˜ì™€ íƒ€ê²Ÿ ì¤€ë¹„
        
        Returns:
            X: í”¼ì²˜
            y: íƒ€ê²Ÿ
            feature_names: í”¼ì²˜ ì´ë¦„ ëª©ë¡
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        """
        logger.info(f"\ní”¼ì²˜ ì¤€ë¹„ (task: {self.task})")
        
        if self.task == "classification":
            return self._prepare_classification(df, target_column, feature_columns)
        elif self.task == "regression":
            return self._prepare_regression(df, target_column, feature_columns)
        elif self.task == "timeseries":
            return self._prepare_timeseries(df, target_column, feature_columns)
        elif self.task == "anomaly_detection":
            return self._prepare_anomaly_detection(df, target_column, feature_columns)
        elif self.task == "clustering":
            return self._prepare_clustering(df, feature_columns)
        else:
            raise ValueError(f"Unknown task: {self.task}")
    
    def _prepare_classification(
        self, df: pd.DataFrame, target_column: str, feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, np.ndarray, List[str], Dict[str, Any]]:
        """ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„"""
        if not target_column or target_column not in df.columns:
            raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        self._log_target_info(df, target_column)
        
        # í”¼ì²˜ ì„ íƒ
        if feature_columns:
            X = df[feature_columns]
        else:
            X = df.drop(columns=[target_column])
        
        y = df[target_column]
        
        # ë¼ë²¨ ì¸ì½”ë”©
        metadata = {}
        if y.dtype == 'object':
            logger.info("ë¼ë²¨ ì¸ì½”ë”© ìˆ˜í–‰")
            y = self._encode_labels(y)
            metadata = {
                "label_encoded": True,
                "original_classes": self.original_classes,
                "label_mapping": self.label_mapping,
                "encoder": self.label_encoder
            }
        else:
            metadata = {"label_encoded": False}
        
        feature_names = X.columns.tolist()
        logger.info(f"í”¼ì²˜ í˜•íƒœ: {X.shape}, íƒ€ê²Ÿ í˜•íƒœ: {y.shape}")
        
        return X, y, feature_names, metadata
    
    def _prepare_regression(
        self, df: pd.DataFrame, target_column: str, feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, np.ndarray, List[str], Dict[str, Any]]:
        """íšŒê·€ ë°ì´í„° ì¤€ë¹„"""
        if not target_column or target_column not in df.columns:
            raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        self._log_target_info(df, target_column)
        
        if feature_columns:
            X = df[feature_columns]
        else:
            X = df.drop(columns=[target_column])
        
        y = df[target_column].values
        feature_names = X.columns.tolist()
        
        metadata = {"label_encoded": False}
        logger.info(f"í”¼ì²˜ í˜•íƒœ: {X.shape}, íƒ€ê²Ÿ í˜•íƒœ: {y.shape}")
        
        return X, y, feature_names, metadata
    
    def _prepare_timeseries(
        self, df: pd.DataFrame, target_column: str, feature_columns: Optional[List[str]]
    ) -> Tuple[np.ndarray, np.ndarray, List[str], Dict[str, Any]]:
        """ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„"""
        if not target_column or target_column not in df.columns:
            raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        lookback_window = self.task_config.get("lookback_window", 10)
        forecast_horizon = self.task_config.get("forecast_horizon", 1)
        time_column = self.task_config.get("time_column")
        
        logger.info(f"ì‹œê³„ì—´ ì„¤ì •: lookback={lookback_window}, horizon={forecast_horizon}")
        
        # ì‹œê°„ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬
        if time_column and time_column in df.columns:
            df = df.sort_values(time_column)
            logger.info(f"ì‹œê°„ ì»¬ëŸ¼ '{time_column}'ìœ¼ë¡œ ì •ë ¬")
        
        # í”¼ì²˜ ì„ íƒ
        if feature_columns:
            feature_cols = feature_columns
        else:
            feature_cols = [col for col in df.columns 
                          if col not in [target_column, time_column]]
        
        # ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±
        X, y = self._create_sequences(
            df[feature_cols].values,
            df[target_column].values,
            lookback_window,
            forecast_horizon
        )
        
        # í”¼ì²˜ ì´ë¦„ ìƒì„± (lag ì •ë³´ í¬í•¨)
        feature_names = []
        for lag in range(lookback_window, 0, -1):
            for col in feature_cols:
                feature_names.append(f"{col}_lag_{lag}")
        
        metadata = {
            "label_encoded": False,
            "time_column": time_column,
            "lookback_window": lookback_window,
            "forecast_horizon": forecast_horizon,
            "original_feature_names": feature_cols,
            "time_series_type": "univariate" if len(feature_cols) == 1 else "multivariate"
        }
        
        logger.info(f"ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: X={X.shape}, y={y.shape}")
        
        return X, y, feature_names, metadata
    
    def _create_sequences(
        self,
        features: np.ndarray,
        target: np.ndarray,
        lookback: int,
        horizon: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
        X, y = [], []
        
        for i in range(len(features) - lookback - horizon + 1):
            # ê³¼ê±° lookback ì‹œì ì˜ ë°ì´í„°
            X.append(features[i:i+lookback].flatten())
            # ë¯¸ë˜ horizon ì‹œì ì˜ íƒ€ê²Ÿ
            if horizon == 1:
                y.append(target[i+lookback])
            else:
                y.append(target[i+lookback:i+lookback+horizon])
        
        return np.array(X), np.array(y)
    
    def _prepare_anomaly_detection(
        self, df: pd.DataFrame, target_column: Optional[str], feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, Optional[np.ndarray], List[str], Dict[str, Any]]:
        """ì´ìƒ íƒì§€ ë°ì´í„° ì¤€ë¹„"""
        contamination = self.task_config.get("contamination", 0.1)
        
        # í”¼ì²˜ ì„ íƒ
        if feature_columns:
            X = df[feature_columns]
        elif target_column and target_column in df.columns:
            X = df.drop(columns=[target_column])
        else:
            X = df.copy()
        
        # íƒ€ê²Ÿ (ìˆëŠ” ê²½ìš°)
        y = None
        is_supervised = False
        
        if target_column and target_column in df.columns:
            y = df[target_column].values
            is_supervised = True
            
            # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°
            actual_contamination = np.sum(y == 1) / len(y) if len(y) > 0 else contamination
            logger.info(f"ì§€ë„ í•™ìŠµ ì´ìƒ íƒì§€ (ë¼ë²¨ ìˆìŒ)")
            logger.info(f"ì´ìƒì¹˜ ë¹„ìœ¨: {actual_contamination:.2%}")
        else:
            logger.info(f"ë¹„ì§€ë„ í•™ìŠµ ì´ìƒ íƒì§€ (ë¼ë²¨ ì—†ìŒ)")
            logger.info(f"ì˜ˆìƒ ì´ìƒì¹˜ ë¹„ìœ¨: {contamination:.2%}")
        
        feature_names = X.columns.tolist()
        
        metadata = {
            "label_encoded": False,
            "is_supervised": is_supervised,
            "contamination": contamination,
            "n_features": len(feature_names)
        }
        
        logger.info(f"í”¼ì²˜ í˜•íƒœ: {X.shape}, íƒ€ê²Ÿ: {'ìˆìŒ' if y is not None else 'ì—†ìŒ'}")
        
        return X, y, feature_names, metadata
    
    def _prepare_clustering(
        self, df: pd.DataFrame, feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, None, List[str], Dict[str, Any]]:
        """í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ì¤€ë¹„"""
        n_clusters = self.task_config.get("n_clusters", 3)
        
        # í”¼ì²˜ ì„ íƒ
        if feature_columns:
            X = df[feature_columns]
        else:
            X = df.copy()
        
        feature_names = X.columns.tolist()
        
        metadata = {
            "label_encoded": False,
            "n_clusters": n_clusters,
            "n_features": len(feature_names)
        }
        
        logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°: {X.shape}, ëª©í‘œ í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
        
        return X, None, feature_names, metadata
    
    def _log_target_info(self, df: pd.DataFrame, target_column: str):
        """íƒ€ê²Ÿ ì»¬ëŸ¼ ì •ë³´ ë¡œê¹…"""
        logger.info(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}' ì •ë³´:")
        logger.info(f"  - ë°ì´í„° íƒ€ì…: {df[target_column].dtype}")
        logger.info(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {df[target_column].nunique()}")
        logger.info(f"  - ê³ ìœ ê°’: {df[target_column].unique()[:10]}")
    
    def _encode_labels(self, y: pd.Series) -> np.ndarray:
        """ë¼ë²¨ ì¸ì½”ë”©"""
        self.original_classes = y.unique().tolist()
        logger.info(f"ì›ë³¸ ë¼ë²¨: {self.original_classes}")
        
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        self.label_mapping = dict(
            zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_)))
        )
        logger.info(f"ë³€í™˜ëœ ë¼ë²¨: {np.unique(y_encoded)}")
        logger.info(f"ë¼ë²¨ ë§¤í•‘: {self.label_mapping}")
        
        return y_encoded
    
    def split_data(
        self,
        X: Any,
        y: Optional[Any],
        test_size: float = 0.2,
        val_size: float = 0.1,
        random_state: int = 42,
    ) -> Tuple:
        """
        íƒœìŠ¤í¬ë³„ ë°ì´í„° ë¶„í• 
        
        Returns:
            X_train, X_val, X_test, y_train, y_val, y_test
        """
        logger.info(f"\në°ì´í„° ë¶„í•  (task: {self.task})")
        logger.info(f"test_size={test_size}, val_size={val_size}")
        
        if self.task == "timeseries":
            return self._split_timeseries(X, y, test_size, val_size)
        elif self.task == "anomaly_detection":
            return self._split_anomaly_detection(X, y, test_size, val_size, random_state)
        elif self.task == "clustering":
            return self._split_clustering(X, test_size, val_size, random_state)
        else:
            # classification, regression
            stratify = y if self.task == "classification" and y is not None else None
            return self._split_standard(X, y, test_size, val_size, random_state, stratify)
    
    def _split_standard(
        self, X, y, test_size, val_size, random_state, stratify
    ) -> Tuple:
        """í‘œì¤€ ë°ì´í„° ë¶„í•  (classification, regression)"""
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=stratify
        )
        logger.info(f"Train+Val: {X_temp.shape}, Test: {X_test.shape}")
        
        if val_size > 0:
            val_ratio = val_size / (1 - test_size)
            stratify_temp = y_temp if stratify is not None else None
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=val_ratio,
                random_state=random_state, stratify=stratify_temp
            )
            logger.info(f"Train: {X_train.shape}, Validation: {X_val.shape}")
        else:
            X_train, X_val, y_train, y_val = X_temp, None, y_temp, None
            logger.info(f"Train: {X_train.shape}, Validation: ì—†ìŒ")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def _split_timeseries(self, X, y, test_size, val_size) -> Tuple:
        """ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  (ì‹œê°„ ìˆœì„œ ìœ ì§€, ì…”í”Œ X)"""
        n_samples = len(X)
        
        # í…ŒìŠ¤íŠ¸ ë¶„í• ì 
        test_split = int(n_samples * (1 - test_size))
        X_temp, X_test = X[:test_split], X[test_split:]
        y_temp, y_test = y[:test_split], y[test_split:]
        
        logger.info(f"ì‹œê³„ì—´ ë¶„í•  (ì‹œê°„ ìˆœì„œ ìœ ì§€): Train+Val={len(X_temp)}, Test={len(X_test)}")
        
        # ê²€ì¦ ë¶„í• 
        if val_size > 0:
            val_split = int(len(X_temp) * (1 - val_size / (1 - test_size)))
            X_train, X_val = X_temp[:val_split], X_temp[val_split:]
            y_train, y_val = y_temp[:val_split], y_temp[val_split:]
            logger.info(f"Train={len(X_train)}, Val={len(X_val)}")
        else:
            X_train, X_val, y_train, y_val = X_temp, None, y_temp, None
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def _split_anomaly_detection(
        self, X, y, test_size, val_size, random_state
    ) -> Tuple:
        """ì´ìƒ íƒì§€ ë°ì´í„° ë¶„í• """
        if y is not None:
            # ì§€ë„ í•™ìŠµ: stratified split
            stratify = y
            return self._split_standard(X, y, test_size, val_size, random_state, stratify)
        else:
            # ë¹„ì§€ë„ í•™ìŠµ: ì¼ë°˜ split (íƒ€ê²Ÿ ì—†ìŒ)
            X_temp, X_test = train_test_split(
                X, test_size=test_size, random_state=random_state
            )
            y_temp, y_test = None, None
            
            if val_size > 0:
                val_ratio = val_size / (1 - test_size)
                X_train, X_val = train_test_split(
                    X_temp, test_size=val_ratio, random_state=random_state
                )
                y_train, y_val = None, None
            else:
                X_train, X_val, y_train, y_val = X_temp, None, None, None
            
            logger.info(f"ë¹„ì§€ë„ ë¶„í• : Train={len(X_train)}, Test={len(X_test)}")
            return X_train, X_val, X_test, y_train, y_val, y_test
    
    def _split_clustering(self, X, test_size, val_size, random_state) -> Tuple:
        """í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¶„í•  (íƒ€ê²Ÿ ì—†ìŒ)"""
        X_temp, X_test = train_test_split(
            X, test_size=test_size, random_state=random_state
        )
        
        if val_size > 0:
            val_ratio = val_size / (1 - test_size)
            X_train, X_val = train_test_split(
                X_temp, test_size=val_ratio, random_state=random_state
            )
        else:
            X_train, X_val = X_temp, None
        
        logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ë¶„í• : Train={len(X_train)}, Test={len(X_test)}")
        return X_train, X_val, X_test, None, None, None
    
    def get_label_encoding_info(self) -> Optional[Dict[str, Any]]:
        """ë¼ë²¨ ì¸ì½”ë”© ì •ë³´ ë°˜í™˜"""
        if self.label_encoder is None:
            return None
        
        return {
            "used": True,
            "original_classes": self.original_classes,
            "label_mapping": self.label_mapping,
            "encoder": self.label_encoder
        }

    # Add to DataLoader class

    def get_input_schema(self, X, feature_names: List[str]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        import pandas as pd
        import numpy as np
        
        schema = {
            "n_features": len(feature_names),
            "feature_names": feature_names,
            "features": []
        }
        
        # ê° í”¼ì²˜ì˜ ë°ì´í„° íƒ€ì… ë° í†µê³„ ì •ë³´
        df = pd.DataFrame(X, columns=feature_names) if not isinstance(X, pd.DataFrame) else X
        
        for col in feature_names:
            feature_info = {
                "name": col,
                "dtype": str(df[col].dtype),
                "nullable": bool(df[col].isnull().any()),
                "null_count": int(df[col].isnull().sum())
            }
            
            # ìˆ˜ì¹˜í˜• ë°ì´í„°
            if pd.api.types.is_numeric_dtype(df[col]):
                feature_info.update({
                    "type": "numeric",
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                    "mean": float(df[col].mean()),
                    "std": float(df[col].std()),
                    "median": float(df[col].median())
                })
            # ë²”ì£¼í˜• ë°ì´í„°
            elif pd.api.types.is_categorical_dtype(df[col]) or df[col].dtype == object:
                unique_vals = df[col].unique()
                feature_info.update({
                    "type": "categorical",
                    "n_unique": len(unique_vals),
                    "categories": unique_vals.tolist()[:100]  # ì²˜ìŒ 100ê°œë§Œ
                })
            
            schema["features"].append(feature_info)
        
        return schema

    def get_output_schema(self, y, task: str, label_encoding_info: Optional[Dict] = None) -> Dict[str, Any]:
        """ì¶œë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        import pandas as pd
        import numpy as np
        
        schema = {
            "type": task,
            "shape": y.shape if hasattr(y, 'shape') else (len(y),)
        }
        
        if task == "classification":
            if label_encoding_info and label_encoding_info.get("used"):
                # ë¼ë²¨ ì¸ì½”ë”©ì´ ì ìš©ëœ ê²½ìš°
                schema.update({
                    "n_classes": len(label_encoding_info["original_classes"]),
                    "class_names": label_encoding_info["original_classes"],
                    "encoded": True,
                    "label_mapping": label_encoding_info["label_mapping"]
                })
            else:
                # ì›ë³¸ ë¼ë²¨
                unique_classes = pd.Series(y).unique().tolist()
                schema.update({
                    "n_classes": len(unique_classes),
                    "class_names": unique_classes,
                    "encoded": False
                })
        
        elif task == "regression":
            y_series = pd.Series(y)
            schema.update({
                "min": float(y_series.min()),
                "max": float(y_series.max()),
                "mean": float(y_series.mean()),
                "std": float(y_series.std())
            })
        
        elif task == "clustering":
            schema.update({
                "n_samples": len(y) if y is not None else 0,
                "unsupervised": y is None
            })
        
        return schema