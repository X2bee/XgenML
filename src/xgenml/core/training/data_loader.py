# /src/xgenml/core/training/data_loader.py
import time
import json
import numpy as np
import pandas as pd
from typing import Tuple, List, Optional, Dict, Any
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from ..data_service import HFDataService, MLflowDataService
from ...utils.logger_config import setup_logger

logger = setup_logger(__name__)


class DataLoader:
    """
    Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ Îã¥Îãπ ÌÅ¥ÎûòÏä§
    ÌÉúÏä§ÌÅ¨Î≥Ñ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ Î°úÏßÅ Î∂ÑÎ¶¨
    """
    
    def __init__(self, task: str, task_config: Optional[Dict[str, Any]] = None):
        """
        Args:
            task: ÌÉúÏä§ÌÅ¨ ÌÉÄÏûÖ (classification, regression, timeseries, anomaly_detection, clustering)
            task_config: ÌÉúÏä§ÌÅ¨Î≥Ñ ÏÑ§Ï†ï
                - timeseries: {"lookback_window": 10, "forecast_horizon": 1, "time_column": "date"}
                - anomaly_detection: {"contamination": 0.1}
                - clustering: {"n_clusters": 3}
        """
        self.task = task
        self.task_config = task_config or {}
        self.hf_service = HFDataService()
        self.mlflow_service = MLflowDataService()
        
        # ÎùºÎ≤® Ïù∏ÏΩîÎî© Í¥ÄÎ†®
        self.label_encoder: Optional[LabelEncoder] = None
        self.label_mapping: Optional[Dict] = None
        self.original_classes: Optional[List] = None
        
        # ÌîºÏ≤ò Ïù∏ÏΩîÎî© Í¥ÄÎ†® (Ï∂îÍ∞Ä)
        self.feature_encoders: Dict[str, LabelEncoder] = {}
        self.feature_encoding_info: Dict[str, Dict] = {}
    
    def load_data(
        self,
        use_mlflow_dataset: bool = False,
        mlflow_run_id: Optional[str] = None,
        mlflow_artifact_path: str = "dataset",
        hf_repo: Optional[str] = None,
        hf_filename: Optional[str] = None,
        hf_revision: Optional[str] = None,
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        logger.info(f"\nüìä Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...")
        load_start_time = time.time()
        
        if use_mlflow_dataset:
            df, data_source_info = self._load_from_mlflow(
                mlflow_run_id, mlflow_artifact_path
            )
        else:
            df, data_source_info = self._load_from_huggingface(
                hf_repo, hf_filename, hf_revision
            )
        
        load_duration = time.time() - load_start_time
        logger.info(f"Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å ({load_duration:.2f}Ï¥à)")
        self._log_data_info(df)
        
        return df, data_source_info
    
    def _load_from_mlflow(
        self, run_id: str, artifact_path: str
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """MLflowÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        if not run_id:
            raise ValueError("MLflow Îç∞Ïù¥ÌÑ∞ÏÖã ÏÇ¨Ïö© Ïãú mlflow_run_idÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§")
        
        logger.info(f"MLflow Îç∞Ïù¥ÌÑ∞ÏÖã ÏÇ¨Ïö©: run_id={run_id}")
        dataset_info = self.mlflow_service.get_dataset_info(run_id)
        logger.info(f"Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥: {dataset_info.get('tags', {}).get('dataset_name', 'Unknown')}")
        
        df = self.mlflow_service.load_dataset_from_run(run_id, artifact_path)
        
        data_source_info = {
            "source_type": "mlflow",
            "mlflow_run_id": run_id,
            "mlflow_artifact_path": artifact_path,
            "dataset_metrics": dataset_info.get('metrics', {}),
        }
        
        return df, data_source_info
    
    def _load_from_huggingface(
        self, repo: str, filename: str, revision: Optional[str]
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """HuggingFaceÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        if not repo or not filename:
            raise ValueError("HuggingFace Îç∞Ïù¥ÌÑ∞ÏÖã ÏÇ¨Ïö© Ïãú hf_repoÏôÄ hf_filenameÏù¥ ÌïÑÏöîÌï©ÎãàÎã§")
        
        logger.info(f"HuggingFace Îç∞Ïù¥ÌÑ∞ÏÖã ÏÇ¨Ïö©: {repo}/{filename}")
        df = self.hf_service.load_dataset_data(repo, filename, revision)
        
        data_source_info = {
            "source_type": "huggingface",
            "hf_repo": repo,
            "hf_filename": filename,
            "hf_revision": revision or "default",
        }
        
        return df, data_source_info
    
    def _log_data_info(self, df: pd.DataFrame):
        """Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥ Î°úÍπÖ"""
        logger.info(f"Îç∞Ïù¥ÌÑ∞ÏÖã ÌòïÌÉú: {df.shape}")
        logger.info(f"Ïª¨Îüº: {df.columns.tolist()}")
        logger.info(f"Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ:\n{df.dtypes}")
        
        null_counts = df.isnull().sum()
        if null_counts.sum() > 0:
            logger.warning(f"Í≤∞Ï∏°Ïπò Î∞úÍ≤¨:\n{null_counts[null_counts > 0]}")
        else:
            logger.info("Í≤∞Ï∏°Ïπò ÏóÜÏùå")
    
    def prepare_features(
        self,
        df: pd.DataFrame,
        target_column: Optional[str] = None,
        feature_columns: Optional[List[str]] = None,
    ) -> Tuple[Any, Any, List[str], Dict[str, Any]]:
        """
        ÌÉúÏä§ÌÅ¨Î≥Ñ ÌîºÏ≤òÏôÄ ÌÉÄÍ≤ü Ï§ÄÎπÑ
        
        Returns:
            X: ÌîºÏ≤ò
            y: ÌÉÄÍ≤ü
            feature_names: ÌîºÏ≤ò Ïù¥Î¶Ñ Î™©Î°ù
            metadata: Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
        """
        logger.info(f"\nÌîºÏ≤ò Ï§ÄÎπÑ (task: {self.task})")
        
        if self.task == "classification":
            return self._prepare_classification(df, target_column, feature_columns)
        elif self.task == "regression":
            return self._prepare_regression(df, target_column, feature_columns)
        elif self.task == "timeseries":
            return self._prepare_timeseries(df, target_column, feature_columns)
        elif self.task == "anomaly_detection":
            return self._prepare_anomaly_detection(df, target_column, feature_columns)
        elif self.task == "clustering":
            return self._prepare_clustering(df, feature_columns)
        else:
            raise ValueError(f"Unknown task: {self.task}")
    
    def _encode_categorical_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """ÌîºÏ≤òÏùò Î≤îÏ£ºÌòï Îç∞Ïù¥ÌÑ∞ Ïù∏ÏΩîÎî©"""
        categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        if not categorical_columns:
            logger.info("Î≤îÏ£ºÌòï ÌîºÏ≤ò ÏóÜÏùå - Ïù∏ÏΩîÎî© Í±¥ÎÑàÎúÄ")
            return X
        
        logger.info(f"üîÑ Î≤îÏ£ºÌòï ÌîºÏ≤ò Ïù∏ÏΩîÎî© ÏãúÏûë: {categorical_columns}")
        
        X_encoded = X.copy()
        
        for col in categorical_columns:
            le = LabelEncoder()
            # Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨
            valid_mask = X[col].notna()
            X_encoded.loc[valid_mask, col] = le.fit_transform(X[col][valid_mask].astype(str))
            
            # Ïù∏ÏΩîÎçî Ï†ÄÏû•
            self.feature_encoders[col] = le
            
            # Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Ï†ÄÏû•
            unique_values = X[col].unique()[:10]  # Ï≤òÏùå 10Í∞úÎßå
            self.feature_encoding_info[col] = {
                "original_values": [str(v) for v in unique_values if pd.notna(v)],
                "encoded_values": le.transform([str(v) for v in unique_values if pd.notna(v)]).tolist(),
                "n_unique": len(le.classes_),
                "classes": le.classes_.tolist()
            }
            
            logger.info(f"  ‚úì {col}: {unique_values[:3]}... ‚Üí Ïà´Ïûê Ïù∏ÏΩîÎî© ({len(le.classes_)} Í≥†Ïú†Í∞í)")
        
        logger.info(f"‚úÖ Î≤îÏ£ºÌòï ÌîºÏ≤ò Ïù∏ÏΩîÎî© ÏôÑÎ£å: {len(categorical_columns)}Í∞ú Ïª¨Îüº")
        
        return X_encoded
    
    def _prepare_classification(
        self, df: pd.DataFrame, target_column: str, feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, np.ndarray, List[str], Dict[str, Any]]:
        """Î∂ÑÎ•ò Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"""
        if not target_column or target_column not in df.columns:
            raise ValueError(f"ÌÉÄÍ≤ü Ïª¨Îüº '{target_column}'Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§")
        
        self._log_target_info(df, target_column)
        
        # ÌîºÏ≤ò ÏÑ†ÌÉù
        if feature_columns:
            X = df[feature_columns].copy()
        else:
            X = df.drop(columns=[target_column]).copy()
        
        # Î≤îÏ£ºÌòï ÌîºÏ≤ò Ïù∏ÏΩîÎî©
        X = self._encode_categorical_features(X)
        
        y = df[target_column]
        
        # ÎùºÎ≤® Ïù∏ÏΩîÎî©
        metadata = {}
        if y.dtype == 'object':
            logger.info("ÌÉÄÍ≤ü ÎùºÎ≤® Ïù∏ÏΩîÎî© ÏàòÌñâ")
            y = self._encode_labels(y)
            metadata = {
                "label_encoded": True,
                "original_classes": self.original_classes,
                "label_mapping": self.label_mapping,
                "encoder": self.label_encoder
            }
        else:
            metadata = {"label_encoded": False}
        
        # ÌîºÏ≤ò Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if self.feature_encoders:
            metadata["feature_encoding"] = self.feature_encoding_info
            metadata["feature_encoders"] = self.feature_encoders
        
        feature_names = X.columns.tolist()
        logger.info(f"ÌîºÏ≤ò ÌòïÌÉú: {X.shape}, ÌÉÄÍ≤ü ÌòïÌÉú: {y.shape}")
        
        return X, y, feature_names, metadata
    
    def _prepare_regression(
        self, df: pd.DataFrame, target_column: str, feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, np.ndarray, List[str], Dict[str, Any]]:
        """ÌöåÍ∑Ä Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"""
        if not target_column or target_column not in df.columns:
            raise ValueError(f"ÌÉÄÍ≤ü Ïª¨Îüº '{target_column}'Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§")
        
        self._log_target_info(df, target_column)
        
        if feature_columns:
            X = df[feature_columns].copy()
        else:
            X = df.drop(columns=[target_column]).copy()
        
        # Î≤îÏ£ºÌòï ÌîºÏ≤ò Ïù∏ÏΩîÎî©
        X = self._encode_categorical_features(X)
        
        y = df[target_column].values
        feature_names = X.columns.tolist()
        
        metadata = {"label_encoded": False}
        
        # ÌîºÏ≤ò Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if self.feature_encoders:
            metadata["feature_encoding"] = self.feature_encoding_info
            metadata["feature_encoders"] = self.feature_encoders
        
        logger.info(f"ÌîºÏ≤ò ÌòïÌÉú: {X.shape}, ÌÉÄÍ≤ü ÌòïÌÉú: {y.shape}")
        
        return X, y, feature_names, metadata
    
    def _prepare_timeseries(
        self, df: pd.DataFrame, target_column: str, feature_columns: Optional[List[str]]
    ) -> Tuple[np.ndarray, np.ndarray, List[str], Dict[str, Any]]:
        """ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"""
        if not target_column or target_column not in df.columns:
            raise ValueError(f"ÌÉÄÍ≤ü Ïª¨Îüº '{target_column}'Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§")
        
        lookback_window = self.task_config.get("lookback_window", 10)
        forecast_horizon = self.task_config.get("forecast_horizon", 1)
        time_column = self.task_config.get("time_column")
        
        logger.info(f"ÏãúÍ≥ÑÏó¥ ÏÑ§Ï†ï: lookback={lookback_window}, horizon={forecast_horizon}")
        
        # ÏãúÍ∞Ñ Ïª¨ÎüºÏúºÎ°ú Ï†ïÎ†¨
        if time_column and time_column in df.columns:
            df = df.sort_values(time_column)
            logger.info(f"ÏãúÍ∞Ñ Ïª¨Îüº '{time_column}'ÏúºÎ°ú Ï†ïÎ†¨")
        
        # ÌîºÏ≤ò ÏÑ†ÌÉù
        if feature_columns:
            feature_cols = feature_columns
        else:
            feature_cols = [col for col in df.columns 
                          if col not in [target_column, time_column]]
        
        # Î≤îÏ£ºÌòï ÌîºÏ≤ò Ïù∏ÏΩîÎî©
        df_features = df[feature_cols].copy()
        df_features = self._encode_categorical_features(df_features)
        
        # ÏãúÍ≥ÑÏó¥ ÏãúÌÄÄÏä§ ÏÉùÏÑ±
        X, y = self._create_sequences(
            df_features.values,
            df[target_column].values,
            lookback_window,
            forecast_horizon
        )
        
        # ÌîºÏ≤ò Ïù¥Î¶Ñ ÏÉùÏÑ± (lag Ï†ïÎ≥¥ Ìè¨Ìï®)
        feature_names = []
        for lag in range(lookback_window, 0, -1):
            for col in df_features.columns:
                feature_names.append(f"{col}_lag_{lag}")
        
        metadata = {
            "label_encoded": False,
            "time_column": time_column,
            "lookback_window": lookback_window,
            "forecast_horizon": forecast_horizon,
            "original_feature_names": feature_cols,
            "time_series_type": "univariate" if len(feature_cols) == 1 else "multivariate"
        }
        
        # ÌîºÏ≤ò Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if self.feature_encoders:
            metadata["feature_encoding"] = self.feature_encoding_info
            metadata["feature_encoders"] = self.feature_encoders
        
        logger.info(f"ÏãúÍ≥ÑÏó¥ ÏãúÌÄÄÏä§ ÏÉùÏÑ± ÏôÑÎ£å: X={X.shape}, y={y.shape}")
        
        return X, y, feature_names, metadata
    
    def _create_sequences(
        self,
        features: np.ndarray,
        target: np.ndarray,
        lookback: int,
        horizon: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ÏãúÍ≥ÑÏó¥ ÏãúÌÄÄÏä§ ÏÉùÏÑ±"""
        X, y = [], []
        
        for i in range(len(features) - lookback - horizon + 1):
            # Í≥ºÍ±∞ lookback ÏãúÏ†êÏùò Îç∞Ïù¥ÌÑ∞
            X.append(features[i:i+lookback].flatten())
            # ÎØ∏Îûò horizon ÏãúÏ†êÏùò ÌÉÄÍ≤ü
            if horizon == 1:
                y.append(target[i+lookback])
            else:
                y.append(target[i+lookback:i+lookback+horizon])
        
        return np.array(X), np.array(y)
    
    def _prepare_anomaly_detection(
        self, df: pd.DataFrame, target_column: Optional[str], feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, Optional[np.ndarray], List[str], Dict[str, Any]]:
        """Ïù¥ÏÉÅ ÌÉêÏßÄ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"""
        contamination = self.task_config.get("contamination", 0.1)
        
        # ÌîºÏ≤ò ÏÑ†ÌÉù
        if feature_columns:
            X = df[feature_columns].copy()
        elif target_column and target_column in df.columns:
            X = df.drop(columns=[target_column]).copy()
        else:
            X = df.copy()
        
        # Î≤îÏ£ºÌòï ÌîºÏ≤ò Ïù∏ÏΩîÎî©
        X = self._encode_categorical_features(X)
        
        # ÌÉÄÍ≤ü (ÏûàÎäî Í≤ΩÏö∞)
        y = None
        is_supervised = False
        
        if target_column and target_column in df.columns:
            y = df[target_column].values
            is_supervised = True
            
            # Ïù¥ÏÉÅÏπò ÎπÑÏú® Í≥ÑÏÇ∞
            actual_contamination = np.sum(y == 1) / len(y) if len(y) > 0 else contamination
            logger.info(f"ÏßÄÎèÑ ÌïôÏäµ Ïù¥ÏÉÅ ÌÉêÏßÄ (ÎùºÎ≤® ÏûàÏùå)")
            logger.info(f"Ïù¥ÏÉÅÏπò ÎπÑÏú®: {actual_contamination:.2%}")
        else:
            logger.info(f"ÎπÑÏßÄÎèÑ ÌïôÏäµ Ïù¥ÏÉÅ ÌÉêÏßÄ (ÎùºÎ≤® ÏóÜÏùå)")
            logger.info(f"ÏòàÏÉÅ Ïù¥ÏÉÅÏπò ÎπÑÏú®: {contamination:.2%}")
        
        feature_names = X.columns.tolist()
        
        metadata = {
            "label_encoded": False,
            "is_supervised": is_supervised,
            "contamination": contamination,
            "n_features": len(feature_names)
        }
        
        # ÌîºÏ≤ò Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if self.feature_encoders:
            metadata["feature_encoding"] = self.feature_encoding_info
            metadata["feature_encoders"] = self.feature_encoders
        
        logger.info(f"ÌîºÏ≤ò ÌòïÌÉú: {X.shape}, ÌÉÄÍ≤ü: {'ÏûàÏùå' if y is not None else 'ÏóÜÏùå'}")
        
        return X, y, feature_names, metadata
    
    def _prepare_clustering(
        self, df: pd.DataFrame, feature_columns: Optional[List[str]]
    ) -> Tuple[pd.DataFrame, None, List[str], Dict[str, Any]]:
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"""
        n_clusters = self.task_config.get("n_clusters", 3)
        
        # ÌîºÏ≤ò ÏÑ†ÌÉù
        if feature_columns:
            X = df[feature_columns].copy()
        else:
            X = df.copy()
        
        # Î≤îÏ£ºÌòï ÌîºÏ≤ò Ïù∏ÏΩîÎî©
        X = self._encode_categorical_features(X)
        
        feature_names = X.columns.tolist()
        
        metadata = {
            "label_encoded": False,
            "n_clusters": n_clusters,
            "n_features": len(feature_names)
        }
        
        # ÌîºÏ≤ò Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if self.feature_encoders:
            metadata["feature_encoding"] = self.feature_encoding_info
            metadata["feature_encoders"] = self.feature_encoders
        
        logger.info(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ Îç∞Ïù¥ÌÑ∞: {X.shape}, Î™©Ìëú ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïàò: {n_clusters}")
        
        return X, None, feature_names, metadata
    
    def _log_target_info(self, df: pd.DataFrame, target_column: str):
        """ÌÉÄÍ≤ü Ïª¨Îüº Ï†ïÎ≥¥ Î°úÍπÖ"""
        logger.info(f"ÌÉÄÍ≤ü Ïª¨Îüº '{target_column}' Ï†ïÎ≥¥:")
        logger.info(f"  - Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: {df[target_column].dtype}")
        logger.info(f"  - Í≥†Ïú†Í∞í Í∞úÏàò: {df[target_column].nunique()}")
        logger.info(f"  - Í≥†Ïú†Í∞í: {df[target_column].unique()[:10]}")
    
    def _encode_labels(self, y: pd.Series) -> np.ndarray:
        """ÎùºÎ≤® Ïù∏ÏΩîÎî©"""
        self.original_classes = y.unique().tolist()
        logger.info(f"ÏõêÎ≥∏ ÎùºÎ≤®: {self.original_classes}")
        
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        self.label_mapping = dict(
            zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_)))
        )
        logger.info(f"Î≥ÄÌôòÎêú ÎùºÎ≤®: {np.unique(y_encoded)}")
        logger.info(f"ÎùºÎ≤® Îß§Ìïë: {self.label_mapping}")
        
        return y_encoded
    
    def split_data(
        self,
        X: Any,
        y: Optional[Any],
        test_size: float = 0.2,
        val_size: float = 0.1,
        random_state: int = 42,
    ) -> Tuple:
        """
        ÌÉúÏä§ÌÅ¨Î≥Ñ Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
        
        Returns:
            X_train, X_val, X_test, y_train, y_val, y_test
        """
        logger.info(f"\nÎç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (task: {self.task})")
        logger.info(f"test_size={test_size}, val_size={val_size}")
        
        if self.task == "timeseries":
            return self._split_timeseries(X, y, test_size, val_size)
        elif self.task == "anomaly_detection":
            return self._split_anomaly_detection(X, y, test_size, val_size, random_state)
        elif self.task == "clustering":
            return self._split_clustering(X, test_size, val_size, random_state)
        else:
            # classification, regression
            stratify = y if self.task == "classification" and y is not None else None
            return self._split_standard(X, y, test_size, val_size, random_state, stratify)
    
    def _split_standard(
        self, X, y, test_size, val_size, random_state, stratify
    ) -> Tuple:
        """ÌëúÏ§Ä Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (classification, regression)"""
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=stratify
        )
        logger.info(f"Train+Val: {X_temp.shape}, Test: {X_test.shape}")
        
        if val_size > 0:
            val_ratio = val_size / (1 - test_size)
            stratify_temp = y_temp if stratify is not None else None
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=val_ratio,
                random_state=random_state, stratify=stratify_temp
            )
            logger.info(f"Train: {X_train.shape}, Validation: {X_val.shape}")
        else:
            X_train, X_val, y_train, y_val = X_temp, None, y_temp, None
            logger.info(f"Train: {X_train.shape}, Validation: ÏóÜÏùå")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def _split_timeseries(self, X, y, test_size, val_size) -> Tuple:
        """ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (ÏãúÍ∞Ñ ÏàúÏÑú Ïú†ÏßÄ, ÏÖîÌîå X)"""
        n_samples = len(X)
        
        # ÌÖåÏä§Ìä∏ Î∂ÑÌï†Ï†ê
        test_split = int(n_samples * (1 - test_size))
        X_temp, X_test = X[:test_split], X[test_split:]
        y_temp, y_test = y[:test_split], y[test_split:]
        
        logger.info(f"ÏãúÍ≥ÑÏó¥ Î∂ÑÌï† (ÏãúÍ∞Ñ ÏàúÏÑú Ïú†ÏßÄ): Train+Val={len(X_temp)}, Test={len(X_test)}")
        
        # Í≤ÄÏ¶ù Î∂ÑÌï†
        if val_size > 0:
            val_split = int(len(X_temp) * (1 - val_size / (1 - test_size)))
            X_train, X_val = X_temp[:val_split], X_temp[val_split:]
            y_train, y_val = y_temp[:val_split], y_temp[val_split:]
            logger.info(f"Train={len(X_train)}, Val={len(X_val)}")
        else:
            X_train, X_val, y_train, y_val = X_temp, None, y_temp, None
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def _split_anomaly_detection(
        self, X, y, test_size, val_size, random_state
    ) -> Tuple:
        """Ïù¥ÏÉÅ ÌÉêÏßÄ Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†"""
        if y is not None:
            # ÏßÄÎèÑ ÌïôÏäµ: stratified split
            stratify = y
            return self._split_standard(X, y, test_size, val_size, random_state, stratify)
        else:
            # ÎπÑÏßÄÎèÑ ÌïôÏäµ: ÏùºÎ∞ò split (ÌÉÄÍ≤ü ÏóÜÏùå)
            X_temp, X_test = train_test_split(
                X, test_size=test_size, random_state=random_state
            )
            y_temp, y_test = None, None
            
            if val_size > 0:
                val_ratio = val_size / (1 - test_size)
                X_train, X_val = train_test_split(
                    X_temp, test_size=val_ratio, random_state=random_state
                )
                y_train, y_val = None, None
            else:
                X_train, X_val, y_train, y_val = X_temp, None, None, None
            
            logger.info(f"ÎπÑÏßÄÎèÑ Î∂ÑÌï†: Train={len(X_train)}, Test={len(X_test)}")
            return X_train, X_val, X_test, y_train, y_val, y_test
    
    def _split_clustering(self, X, test_size, val_size, random_state) -> Tuple:
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (ÌÉÄÍ≤ü ÏóÜÏùå)"""
        X_temp, X_test = train_test_split(
            X, test_size=test_size, random_state=random_state
        )
        
        if val_size > 0:
            val_ratio = val_size / (1 - test_size)
            X_train, X_val = train_test_split(
                X_temp, test_size=val_ratio, random_state=random_state
            )
        else:
            X_train, X_val = X_temp, None
        
        logger.info(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ Î∂ÑÌï†: Train={len(X_train)}, Test={len(X_test)}")
        return X_train, X_val, X_test, None, None, None
    
    def get_label_encoding_info(self) -> Optional[Dict[str, Any]]:
        """ÎùºÎ≤® Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Î∞òÌôò"""
        if self.label_encoder is None:
            return None
        
        return {
            "used": True,
            "original_classes": self.original_classes,
            "label_mapping": self.label_mapping,
            "encoder": self.label_encoder
        }

    def get_input_schema(self, X, feature_names: List[str]) -> Dict[str, Any]:
        """ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ïä§ÌÇ§Îßà ÏÉùÏÑ±"""
        import pandas as pd
        import numpy as np
        
        schema = {
            "n_features": len(feature_names),
            "feature_names": feature_names,
            "description": f"Input schema for {self.task} task with {len(feature_names)} features",
            "features": []
        }
        
        # Í∞Å ÌîºÏ≤òÏùò Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î∞è ÌÜµÍ≥Ñ Ï†ïÎ≥¥
        df = pd.DataFrame(X, columns=feature_names) if not isinstance(X, pd.DataFrame) else X
        
        for col in feature_names:
            feature_info = {
                "name": col,
                "dtype": str(df[col].dtype),
                "nullable": bool(df[col].isnull().any()),
                "null_count": int(df[col].isnull().sum())
            }
            
            # Î≤îÏ£ºÌòï Ïù∏ÏΩîÎî© Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if col in self.feature_encoding_info:
                feature_info["encoding"] = {
                    "type": "label_encoded",
                    "original_type": "categorical",
                    "description": f"Categorical feature '{col}' encoded to numeric",
                    **self.feature_encoding_info[col]
                }
            
            # ÏàòÏπòÌòï Îç∞Ïù¥ÌÑ∞
            if pd.api.types.is_numeric_dtype(df[col]):
                feature_info.update({
                    "type": "numeric",
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                    "mean": float(df[col].mean()),
                    "std": float(df[col].std()),
                    "median": float(df[col].median()),
                    "description": f"Numeric feature with range [{df[col].min():.2f}, {df[col].max():.2f}]"
                })
            # Î≤îÏ£ºÌòï Îç∞Ïù¥ÌÑ∞
            elif pd.api.types.is_categorical_dtype(df[col]) or df[col].dtype == object:
                unique_vals = df[col].unique()
                feature_info.update({
                    "type": "categorical",
                    "n_unique": len(unique_vals),
                    "categories": unique_vals.tolist()[:100],  # Ï≤òÏùå 100Í∞úÎßå
                    "description": f"Categorical feature with {len(unique_vals)} unique values"
                })
            
            schema["features"].append(feature_info)
        
        return schema

    def get_output_schema(self, y, task: str, label_encoding_info: Optional[Dict] = None) -> Dict[str, Any]:
        """Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ Ïä§ÌÇ§Îßà ÏÉùÏÑ±"""
        import pandas as pd
        import numpy as np
        
        schema = {
            "type": task,
            "shape": y.shape if hasattr(y, 'shape') else (len(y),),
            "description": f"Output schema for {task} task"
        }
        
        if task == "classification":
            if label_encoding_info and label_encoding_info.get("used"):
                # ÎùºÎ≤® Ïù∏ÏΩîÎî©Ïù¥ Ï†ÅÏö©Îêú Í≤ΩÏö∞
                schema.update({
                    "n_classes": len(label_encoding_info["original_classes"]),
                    "class_names": label_encoding_info["original_classes"],
                    "encoded": True,
                    "label_mapping": label_encoding_info["label_mapping"],
                    "description": f"Classification output with {len(label_encoding_info['original_classes'])} classes. Original labels: {label_encoding_info['original_classes']}"
                })
            else:
                # ÏõêÎ≥∏ ÎùºÎ≤®
                unique_classes = pd.Series(y).unique().tolist()
                schema.update({
                    "n_classes": len(unique_classes),
                    "class_names": unique_classes,
                    "encoded": False,
                    "description": f"Classification output with {len(unique_classes)} classes: {unique_classes}"
                })
        
        elif task == "regression":
            y_series = pd.Series(y)
            schema.update({
                "min": float(y_series.min()),
                "max": float(y_series.max()),
                "mean": float(y_series.mean()),
                "std": float(y_series.std()),
                "description": f"Regression output - continuous numeric values in range [{y_series.min():.2f}, {y_series.max():.2f}]"
            })
        
        elif task == "timeseries":
            y_series = pd.Series(y.flatten()) if hasattr(y, 'flatten') else pd.Series(y)
            schema.update({
                "min": float(y_series.min()),
                "max": float(y_series.max()),
                "mean": float(y_series.mean()),
                "description": f"Time series forecast output - predicting future values with horizon={self.task_config.get('forecast_horizon', 1)}"
            })
        
        elif task == "anomaly_detection":
            schema.update({
                "description": "Anomaly detection output - binary classification (0: normal, 1: anomaly)",
                "contamination": self.task_config.get("contamination", 0.1)
            })
        
        elif task == "clustering":
            schema.update({
                "n_samples": len(y) if y is not None else 0,
                "unsupervised": y is None,
                "n_clusters": self.task_config.get("n_clusters", 3),
                "description": f"Clustering output - assigns samples to {self.task_config.get('n_clusters', 3)} clusters"
            })
        
        return schema