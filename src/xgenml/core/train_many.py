# /src/xgenml/core/train_many.py
import os
import uuid
import json
import tempfile
import joblib
import mlflow
import mlflow.sklearn
import logging
import traceback
import time
import numpy as np
from datetime import datetime
from typing import List, Dict, Any, Optional
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from mlflow.tracking import MlflowClient

from .data_service import HFDataService
from .model_provider import create_estimator
from .metrics import classification_metrics, regression_metrics
from .storage import StorageClient
from ..services.hyperparameter_optimization import HyperparameterOptimizer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/tmp/training.log', mode='a', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

PRIMARY_METRIC = {"classification": "accuracy", "regression": "r2"}


def create_safe_directory(base_path: str, identifier: str) -> str:
    """
    ì•ˆì „í•˜ê²Œ ê³ ìœ í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    timestamp = int(time.time())
    unique_id = uuid.uuid4().hex[:8]
    safe_dir = f"{base_path}/{identifier}_{timestamp}_{unique_id}"
    
    try:
        os.makedirs(safe_dir, exist_ok=True, mode=0o755)
        # ê¶Œí•œ í™•ì¸
        test_file = os.path.join(safe_dir, "test_write")
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)
        logger.info(f"âœ… ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìƒì„±: {safe_dir}")
        return safe_dir
    except Exception as e:
        logger.error(f"âŒ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ì™„ì „íˆ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        fallback_dir = tempfile.mkdtemp(prefix=f"mlflow_{identifier}_")
        logger.info(f"ğŸ”„ í´ë°± ë””ë ‰í† ë¦¬ ì‚¬ìš©: {fallback_dir}")
        return fallback_dir


def _register_best_to_mlflow(model_name: str, best: dict, feature_names: list[str], version_tags: Optional[Dict[str, str]] = None) -> Optional[str]:
    """
    best['run_id']ê°€ ê°€ë¦¬í‚¤ëŠ” runì˜ 'model' ì•„í‹°íŒ©íŠ¸ë¥¼
    MLflow Model Registryì— ìƒˆ ë²„ì „ìœ¼ë¡œ ë“±ë¡í•˜ê³  Productionìœ¼ë¡œ ìŠ¹ê²©.
    feature_namesëŠ” ëª¨ë¸ ë²„ì „ íƒœê·¸ë¡œ ì €ì¥.
    ë²„ì „ë³„ íƒœê·¸ ì¶”ê°€ ì§€ì›.
    ë°˜í™˜: ë“±ë¡ëœ ëª¨ë¸ ë²„ì „ ë¬¸ìì—´(ì˜ˆ: '3'), ì‹¤íŒ¨ ì‹œ None
    """
    logger.info(f"ğŸ·ï¸  Model Registry ë“±ë¡ ì‹œì‘: {model_name}")
    
    try:
        client = MlflowClient()
        
        # MLflow ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
        logger.info("MLflow ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸...")
        try:
            client.search_experiments(max_results=1)
            logger.info("âœ… MLflow ì„œë²„ ì—°ê²° ì„±ê³µ")
        except Exception as conn_error:
            logger.error(f"âŒ MLflow ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {conn_error}")
            return None
        
        run_id = best["run_id"]
        logger.info(f"Run ID: {run_id}")

        # run.artifact_uri + '/model' ì´ ëª¨ë¸ ì†ŒìŠ¤ ê²½ë¡œ
        logger.info("Run ì •ë³´ ì¡°íšŒ ì¤‘...")
        run = client.get_run(run_id)
        source = f"{run.info.artifact_uri}/model"
        logger.info(f"ëª¨ë¸ ì†ŒìŠ¤ ê²½ë¡œ: {source}")

        # ë“±ë¡ ëª¨ë¸ ë³´ì¥
        logger.info(f"ë“±ë¡ ëª¨ë¸ í™•ì¸/ìƒì„±: {model_name}")
        try:
            registered_model = client.get_registered_model(model_name)
            logger.info(f"âœ… ê¸°ì¡´ ë“±ë¡ ëª¨ë¸ ë°œê²¬: {model_name}")
            logger.info(f"ëª¨ë¸ ì„¤ëª…: {registered_model.description or 'None'}")
        except Exception as e:
            logger.info(f"ìƒˆ ë“±ë¡ ëª¨ë¸ ìƒì„±: {model_name}")
            client.create_registered_model(
                model_name,
                description=f"Auto-generated model registry for {model_name}"
            )
            logger.info("âœ… ìƒˆ ë“±ë¡ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

        # ë²„ì „ ìƒì„±
        logger.info("ëª¨ë¸ ë²„ì „ ìƒì„± ì¤‘...")
        mv = client.create_model_version(name=model_name, source=source, run_id=run_id)
        logger.info(f"âœ… ëª¨ë¸ ë²„ì „ ìƒì„± ì™„ë£Œ: v{mv.version}")

        # íƒœê·¸ë¡œ feature_names ì €ì¥
        logger.info("í”¼ì²˜ ì´ë¦„ íƒœê·¸ ì €ì¥ ì¤‘...")
        feature_names_json = json.dumps(feature_names, ensure_ascii=False)
        client.set_model_version_tag(
            name=model_name,
            version=mv.version,
            key="feature_names",
            value=feature_names_json,
        )
        
        # ë²„ì „ ê´€ë¦¬ íƒœê·¸ë“¤ ì¶”ê°€
        version_info_tags = {
            "created_at": datetime.now().isoformat(),
            "training_run_id": run_id,
            "algorithm": best.get("algorithm", "unknown"),
            "best_metric": json.dumps({
                "name": PRIMARY_METRIC.get(best.get("task", "classification"), "accuracy"),
                "value": best.get("metrics", {}).get("test", {}).get(PRIMARY_METRIC.get(best.get("task", "classification"), "accuracy"), 0.0)
            }),
        }
        
        # ì‚¬ìš©ì ì •ì˜ íƒœê·¸ ì¶”ê°€
        if version_tags:
            version_info_tags.update(version_tags)
        
        # ëª¨ë“  íƒœê·¸ ì„¤ì •
        for tag_key, tag_value in version_info_tags.items():
            client.set_model_version_tag(
                name=model_name,
                version=mv.version,
                key=tag_key,
                value=str(tag_value),
            )
        
        logger.info(f"âœ… íƒœê·¸ ì €ì¥ ì™„ë£Œ: {len(feature_names)}ê°œ í”¼ì²˜ + {len(version_info_tags)}ê°œ ë²„ì „ íƒœê·¸")

        # ìŠ¹ê²©(Production), ê¸°ì¡´ Productionì€ archive
        logger.info(f"ëª¨ë¸ v{mv.version}ì„ Productionìœ¼ë¡œ ìŠ¹ê²© ì¤‘...")
        client.transition_model_version_stage(
            name=model_name,
            version=mv.version,
            stage="Production",
            archive_existing_versions=True,
        )
        logger.info(f"ğŸ‰ ëª¨ë¸ v{mv.version} Production ìŠ¹ê²© ì™„ë£Œ!")
        
        return mv.version
        
    except Exception as e:
        logger.error(f"âŒ Model Registry ë“±ë¡ ì‹¤íŒ¨: {str(e)}")
        logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        logger.error(f"ìƒì„¸ ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None


def train_from_hf(
    model_id: str,
    task: str,
    hf_repo: str,
    hf_filename: str,
    hf_revision: Optional[str],
    target_column: str,
    feature_columns: Optional[List[str]],
    model_names: List[str],
    overrides: Optional[Dict[str, Dict[str, Any]]] = None,
    test_size: float = 0.2,
    val_size: float = 0.1,
    random_state: int = 42,
    use_cv: bool = False,
    cv_folds: int = 5,
    mlflow_experiment: Optional[str] = None,
    artifact_base_uri: Optional[str] = None,
    storage_ctor_kwargs: Optional[Dict[str, Any]] = None,
    hpo_config: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    ì „ì²˜ë¦¬ ì™„ë£Œ ë°ì´í„° ê°€ì • / ì—¬ëŸ¬ ëª¨ë¸ ì¼ê´„ í•™ìŠµÂ·í‰ê°€Â·ë¡œê¹….
    - ê³ ìœ í•œ ì‹¤í–‰ IDë¡œ ì¶©ëŒ ë°©ì§€
    - ë² ìŠ¤íŠ¸ runì„ MLflow Model Registryì— ë“±ë¡í•˜ê³  Productionìœ¼ë¡œ ìŠ¹ê²©
    - feature_namesë¥¼ ëª¨ë¸ ë²„ì „ íƒœê·¸ë¡œ ì €ì¥
    - run ì•„í‹°íŒ©íŠ¸ë¡œ manifest.jsonë„ ë‚¨ê¹€(ë°±ì—…/ì°¸ì¡°ìš©)
    - (ì„ íƒ) ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ì—ë„ manifest ì—…ë¡œë“œ
    - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì§€ì›
    - ìë™ ë¼ë²¨ ì¸ì½”ë”© ì§€ì›
    - ê°œì„ ëœ ë²„ì „ ê´€ë¦¬ ë° íƒœê¹…
    """
    training_start_time = time.time()
    
    # ê³ ìœ í•œ ì‹¤í–‰ ID ìƒì„± (ì¶©ëŒ ë°©ì§€)
    execution_id = f"{model_id}_{uuid.uuid4().hex[:8]}_{int(time.time())}"
    
    logger.info("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    logger.info("=" * 80)
    logger.info(f"ğŸ†” ê³ ìœ  ì‹¤í–‰ ID: {execution_id}")
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ë™ì‘ ì œì–´
    USE_UNIQUE_PATHS = os.getenv("MLFLOW_USE_UNIQUE_PATHS", "true").lower() == "true"
    CLEANUP_TEMP_FILES = os.getenv("MLFLOW_CLEANUP_TEMP", "true").lower() == "true"
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ê´€ë¦¬
    temp_directories = []
    
    try:
        # ì…ë ¥ íŒŒë¼ë¯¸í„° ë¡œê¹…
        logger.info(f"ğŸ“‹ í•™ìŠµ ì„¤ì •:")
        logger.info(f"  - Model ID: {model_id}")
        logger.info(f"  - Execution ID: {execution_id}")
        logger.info(f"  - Task: {task}")
        logger.info(f"  - HF Repo: {hf_repo}")
        logger.info(f"  - HF Filename: {hf_filename}")
        logger.info(f"  - HF Revision: {hf_revision or 'default'}")
        logger.info(f"  - Target Column: {target_column}")
        logger.info(f"  - Feature Columns: {feature_columns or 'auto (all except target)'}")
        logger.info(f"  - Model Names: {model_names}")
        logger.info(f"  - Test Size: {test_size}")
        logger.info(f"  - Validation Size: {val_size}")
        logger.info(f"  - Use CV: {use_cv} ({cv_folds} folds)")
        logger.info(f"  - Random State: {random_state}")
        logger.info(f"  - Use Unique Paths: {USE_UNIQUE_PATHS}")
        if overrides:
            logger.info(f"  - Overrides: {overrides}")
        if hpo_config:
            logger.info(f"  - HPO Config: {hpo_config}")
        
        # MLflow ì„¤ì •
        logger.info("\nğŸ”§ MLflow ì„¤ì • ì¤‘...")
        tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "")
        if not tracking_uri:
            logger.warning("âš ï¸  MLFLOW_TRACKING_URI í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            raise ValueError("MLFLOW_TRACKING_URI í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        logger.info(f"MLflow Tracking URI: {tracking_uri}")
        mlflow.set_tracking_uri(tracking_uri)
        
        # ì•„í‹°íŒ©íŠ¸ ê²½ë¡œ ì„¤ì • - ì¶©ëŒ ë°©ì§€
        logger.info("MLflow ì•„í‹°íŒ©íŠ¸ ì„¤ì • ì¤‘...")
        
        if tracking_uri.startswith("http"):
            # ì›ê²© ì„œë²„ ì‚¬ìš©
            artifact_uri = f"{tracking_uri.rstrip('/')}/api/2.0/mlflow-artifacts/artifacts"
            os.environ["MLFLOW_DEFAULT_ARTIFACT_ROOT"] = artifact_uri
            logger.info(f"ì›ê²© ì•„í‹°íŒ©íŠ¸ URI: {artifact_uri}")
        else:
            # ë¡œì»¬ ì‚¬ìš©ì‹œ ê³ ìœ  ê²½ë¡œ (ì¶©ëŒ ë°©ì§€)
            if USE_UNIQUE_PATHS:
                safe_local_path = create_safe_directory("/app/mlruns", execution_id)
                temp_directories.append(safe_local_path)
                os.environ["MLFLOW_DEFAULT_ARTIFACT_ROOT"] = safe_local_path
                logger.info(f"ë¡œì»¬ ê³ ìœ  ì•„í‹°íŒ©íŠ¸ ê²½ë¡œ: {safe_local_path}")
            else:
                artifact_uri = f"{tracking_uri.rstrip('/')}/api/2.0/mlflow-artifacts/artifacts"
                os.environ["MLFLOW_DEFAULT_ARTIFACT_ROOT"] = artifact_uri
                logger.info(f"ê¸°ë³¸ ì•„í‹°íŒ©íŠ¸ URI: {artifact_uri}")
        
        # ì‹¤í—˜ ì´ë¦„ë„ ê³ ìœ í•˜ê²Œ (ì„ íƒì )
        if USE_UNIQUE_PATHS:
            experiment_name = mlflow_experiment or f"model_{execution_id}"
        else:
            experiment_name = mlflow_experiment or f"model_{model_id}"
        
        logger.info(f"ì‹¤í—˜ ì´ë¦„: {experiment_name}")
        
        # ì‹¤í—˜ ìƒì„±/ì„¤ì •
        try:
            experiment_id = mlflow.create_experiment(
                experiment_name,
                artifact_location=f"mlflow-artifacts:/{experiment_name}"
            )
            logger.info(f"âœ… ìƒˆ ì‹¤í—˜ ìƒì„±: {experiment_name}")
        except Exception:
            experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id
            logger.info(f"âœ… ê¸°ì¡´ ì‹¤í—˜ ì‚¬ìš©: {experiment_name}")
        
        mlflow.set_experiment(experiment_id=experiment_id)
        logger.info("âœ… MLflow ì„¤ì • ì™„ë£Œ")

        # ë°ì´í„° ë¡œë“œ
        logger.info(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: {hf_repo}/{hf_filename}")
        load_start_time = time.time()
        
        try:
            df = HFDataService().load_dataset_data(hf_repo, hf_filename, hf_revision)
            load_duration = time.time() - load_start_time
            logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({load_duration:.2f}ì´ˆ)")
            logger.info(f"ë°ì´í„°ì…‹ í˜•íƒœ: {df.shape}")
            logger.info(f"ì»¬ëŸ¼: {df.columns.tolist()}")
            logger.info(f"ë°ì´í„° íƒ€ì…:\n{df.dtypes}")
            
            # ê²°ì¸¡ì¹˜ í™•ì¸
            null_counts = df.isnull().sum()
            if null_counts.sum() > 0:
                logger.warning(f"âš ï¸  ê²°ì¸¡ì¹˜ ë°œê²¬:\n{null_counts[null_counts > 0]}")
            else:
                logger.info("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            raise

        # íƒ€ê²Ÿ ì»¬ëŸ¼ í™•ì¸
        if target_column not in df.columns:
            error_msg = f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ ë°ì´í„°ì…‹ì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {df.columns.tolist()}"
            logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        logger.info(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}' ì •ë³´:")
        logger.info(f"  - ë°ì´í„° íƒ€ì…: {df[target_column].dtype}")
        logger.info(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {df[target_column].nunique()}")
        logger.info(f"  - ê³ ìœ ê°’: {df[target_column].unique()[:10]}")
        
        # í”¼ì²˜ ì„¤ì •
        if feature_columns:
            missing_features = [col for col in feature_columns if col not in df.columns]
            if missing_features:
                error_msg = f"í”¼ì²˜ ì»¬ëŸ¼ì´ ë°ì´í„°ì…‹ì— ì—†ìŠµë‹ˆë‹¤: {missing_features}"
                logger.error(f"âŒ {error_msg}")
                raise ValueError(error_msg)
            X = df[feature_columns]
            logger.info(f"âœ… ì§€ì •ëœ í”¼ì²˜ ì»¬ëŸ¼ ì‚¬ìš©: {len(feature_columns)}ê°œ")
        else:
            X = df.drop(columns=[target_column])
            logger.info(f"âœ… ìë™ í”¼ì²˜ ì„ íƒ: íƒ€ê²Ÿ ì œì™¸ {X.shape[1]}ê°œ ì»¬ëŸ¼")
        
        y = df[target_column]
        logger.info(f"ìµœì¢… í”¼ì²˜ í˜•íƒœ: {X.shape}")
        logger.info(f"íƒ€ê²Ÿ í˜•íƒœ: {y.shape}")

        # ë¼ë²¨ ì¸ì½”ë”© ì²˜ë¦¬ (ë¶„ë¥˜ íƒœìŠ¤í¬ì´ê³  íƒ€ê²Ÿì´ ë¬¸ìì—´ì¸ ê²½ìš°)
        label_encoder = None
        original_classes = None
        label_mapping = None
        
        if task == "classification" and y.dtype == 'object':
            logger.info(f"\nğŸ·ï¸  íƒ€ê²Ÿ ë¼ë²¨ì´ ë¬¸ìì—´ì…ë‹ˆë‹¤. ìë™ìœ¼ë¡œ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            original_classes = y.unique().tolist()
            logger.info(f"ì›ë³¸ ë¼ë²¨: {original_classes}")
            
            label_encoder = LabelEncoder()
            y_encoded = label_encoder.fit_transform(y)
            
            # ë¼ë²¨ ë§¤í•‘ ì •ë³´
            label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))
            logger.info(f"ë³€í™˜ëœ ë¼ë²¨: {np.unique(y_encoded)}")
            logger.info(f"ë¼ë²¨ ë§¤í•‘: {label_mapping}")
            
            # y ê°’ë“¤ì„ ì¸ì½”ë”©ëœ ê°’ìœ¼ë¡œ êµì²´
            y = y_encoded
            logger.info("âœ… ë¼ë²¨ ì¸ì½”ë”© ì™„ë£Œ")

        # ë°ì´í„° ë¶„í• 
        logger.info(f"\nğŸ”€ ë°ì´í„° ë¶„í•  (test_size={test_size}, val_size={val_size})")
        try:
            X_temp, X_test, y_temp, y_test = train_test_split(
                X,
                y,
                test_size=test_size,
                random_state=random_state,
                stratify=y if task == "classification" else None,
            )
            logger.info(f"Train+Val: {X_temp.shape}, Test: {X_test.shape}")
            
            if val_size > 0:
                val_ratio = val_size / (1 - test_size)
                X_train, X_val, y_train, y_val = train_test_split(
                    X_temp,
                    y_temp,
                    test_size=val_ratio,
                    random_state=random_state,
                    stratify=y_temp if task == "classification" else None,
                )
                logger.info(f"Train: {X_train.shape}, Validation: {X_val.shape}")
            else:
                X_train, X_val, y_train, y_val = X_temp, None, y_temp, None
                logger.info(f"Train: {X_train.shape}, Validation: ì—†ìŒ")
            
            logger.info("âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            raise

        # í•™ìŠµì— ì‚¬ìš©ëœ í”¼ì²˜ ì´ë¦„
        used_feature_names = X.columns.tolist()
        logger.info(f"ì‚¬ìš©ëœ í”¼ì²˜ ì´ë¦„: {used_feature_names}")

        # HPO ì„¤ì •
        use_hpo = hpo_config and hpo_config.get('enable_hpo', False)
        optimizer = None
        if use_hpo:
            logger.info("\nğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í™œì„±í™”")
            optimizer = HyperparameterOptimizer(
                n_trials=hpo_config.get('n_trials', 50),
                timeout=hpo_config.get('timeout_minutes', None) * 60 if hpo_config.get('timeout_minutes') else None
            )
            logger.info(f"  - Trials: {hpo_config.get('n_trials', 50)}")
            logger.info(f"  - Timeout: {hpo_config.get('timeout_minutes', 'None')} minutes")

        results: List[Dict[str, Any]] = []
        best: Optional[Dict[str, Any]] = None
        best_key = PRIMARY_METRIC[task]
        best_score = -1e18 if task == "regression" else -1.0
        
        logger.info(f"\nğŸ¤– ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({len(model_names)}ê°œ ëª¨ë¸)")
        logger.info(f"í‰ê°€ ì§€í‘œ: {best_key}")

        # ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ë°˜ë³µ
        for idx, name in enumerate(model_names, 1):
            model_start_time = time.time()
            logger.info(f"\n[{idx}/{len(model_names)}] {name} í•™ìŠµ ì¤‘...")
            
            try:
                # ê³ ìœ í•œ run ì´ë¦„ ìƒì„± (ì¶©ëŒ ë°©ì§€)
                if USE_UNIQUE_PATHS:
                    run_name = f"{task}:{name}:{execution_id[-8:]}"
                else:
                    run_name = f"{task}:{name}"
                
                with mlflow.start_run(run_name=run_name):
                    run_id = mlflow.active_run().info.run_id
                    logger.info(f"MLflow Run ID: {run_id}")
                    
                    # ì‹¤í–‰ IDë¥¼ MLflowì—ë„ ì €ì¥
                    mlflow.log_param("execution_id", execution_id)
                    mlflow.log_param("unique_run", USE_UNIQUE_PATHS)
                    
                    # ë¼ë²¨ ì¸ì½”ë”© ì •ë³´ë¥¼ MLflowì— ì €ì¥
                    if label_encoder is not None:
                        mlflow.log_param("label_encoded", True)
                        mlflow.log_param("original_classes", json.dumps(original_classes, ensure_ascii=False))
                        mlflow.log_param("label_mapping", json.dumps(label_mapping, ensure_ascii=False))
                        
                        # ë¼ë²¨ ì¸ì½”ë” ìì²´ë„ ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥
                        label_encoder_path = f"/tmp/{run_id}_label_encoder.pkl"
                        joblib.dump(label_encoder, label_encoder_path)
                        mlflow.log_artifact(label_encoder_path, artifact_path="preprocessing")
                        os.unlink(label_encoder_path)  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                        
                        logger.info("ğŸ“‹ ë¼ë²¨ ì¸ì½”ë”© ì •ë³´ë¥¼ MLflowì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        mlflow.log_param("label_encoded", False)
                    
                    # ëª¨ë¸ ìƒì„± - HPO ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
                    model_params = {}
                    hpo_results = None
                    
                    if use_hpo and optimizer._get_default_param_space(name, task):
                        logger.info(f"ğŸ¯ {name} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
                        
                        # ì‚¬ìš©ì ì •ì˜ íŒŒë¼ë¯¸í„° ê³µê°„ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
                        custom_param_space = None
                        if hpo_config.get('param_spaces') and name in hpo_config['param_spaces']:
                            custom_param_space = hpo_config['param_spaces'][name]
                            logger.info(f"ì‚¬ìš©ì ì •ì˜ íŒŒë¼ë¯¸í„° ê³µê°„ ì‚¬ìš©: {custom_param_space}")
                        
                        # HPO ì‹¤í–‰
                        hpo_results = optimizer.optimize_model(
                            model_name=name,
                            task=task,
                            X_train=X_train,
                            y_train=y_train,
                            X_val=X_val,
                            y_val=y_val,
                            cv_folds=cv_folds,
                            param_space=custom_param_space
                        )
                        
                        # ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
                        model_params = hpo_results['best_params']
                        logger.info(f"âœ… HPO ì™„ë£Œ - ìµœì  íŒŒë¼ë¯¸í„°: {model_params}")
                        logger.info(f"HPO ìµœê³  ì ìˆ˜: {hpo_results['best_score']:.4f}")
                        
                        # HPO ê²°ê³¼ë¥¼ MLflowì— ë¡œê¹…
                        mlflow.log_params({f"hpo_{k}": v for k, v in model_params.items()})
                        mlflow.log_metric("hpo_best_score", hpo_results['best_score'])
                        mlflow.log_metric("hpo_n_trials", hpo_results['n_trials'])
                        mlflow.log_param("hpo_enabled", True)
                        
                    else:
                        # ê¸°ì¡´ ë°©ì‹: ê¸°ë³¸ íŒŒë¼ë¯¸í„° + ì˜¤ë²„ë¼ì´ë“œ
                        if use_hpo and not optimizer._get_default_param_space(name, task):
                            logger.info(f"âš ï¸  {name}ì— ëŒ€í•œ HPO íŒŒë¼ë¯¸í„° ê³µê°„ì´ ì •ì˜ë˜ì§€ ì•ŠìŒ. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.")
                        
                        model_overrides = (overrides or {}).get(name, {})
                        model_params = model_overrides
                        mlflow.log_param("hpo_enabled", False)
                        if model_overrides:
                            logger.info(f"ì‚¬ìš©ì ì˜¤ë²„ë¼ì´ë“œ íŒŒë¼ë¯¸í„°: {model_overrides}")
                    
                    # ëª¨ë¸ ìƒì„±
                    logger.info("ëª¨ë¸ ìƒì„± ì¤‘...")
                    est = create_estimator(task, name, model_params)
                    logger.info(f"ëª¨ë¸ ìƒì„± ì™„ë£Œ: {type(est).__name__}")
                    
                    # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë¡œê¹…
                    params_to_log = {
                        "algorithm": name,
                        "test_size": test_size,
                        "val_size": val_size,
                        "use_cv": use_cv,
                        "cv_folds": cv_folds,
                        "hf_repo": hf_repo,
                        "hf_filename": hf_filename,
                        "hf_revision": hf_revision or "default",
                    }
                    params_to_log.update(model_params)
                    
                    mlflow.log_params(params_to_log)
                    logger.info("íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ")

                    # ëª¨ë¸ í•™ìŠµ
                    logger.info("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
                    fit_start_time = time.time()
                    est.fit(X_train, y_train)
                    fit_duration = time.time() - fit_start_time
                    logger.info(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ({fit_duration:.2f}ì´ˆ)")

                    # í‰ê°€
                    logger.info("ëª¨ë¸ í‰ê°€ ì¤‘...")
                    metrics: Dict[str, Any] = {}
                    
                    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
                    y_pred_test = est.predict(X_test)
                    if task == "classification":
                        metrics["test"] = classification_metrics(y_test, y_pred_test)
                    else:
                        metrics["test"] = regression_metrics(y_test, y_pred_test)
                    
                    test_score = metrics["test"][best_key]
                    logger.info(f"í…ŒìŠ¤íŠ¸ {best_key}: {test_score:.4f}")

                    # ê²€ì¦ ì„¸íŠ¸ í‰ê°€
                    if X_val is not None:
                        y_pred_val = est.predict(X_val)
                        if task == "classification":
                            metrics["validation"] = classification_metrics(y_val, y_pred_val)
                        else:
                            metrics["validation"] = regression_metrics(y_val, y_pred_val)
                        
                        val_score = metrics["validation"][best_key]
                        logger.info(f"ê²€ì¦ {best_key}: {val_score:.4f}")

                    # êµì°¨ ê²€ì¦
                    if use_cv:
                        logger.info(f"êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì¤‘ ({cv_folds} folds)...")
                        cv_start_time = time.time()
                        scoring = "accuracy" if task == "classification" else "r2"
                        scores = cross_val_score(est, X_train, y_train, cv=cv_folds, scoring=scoring)
                        cv_duration = time.time() - cv_start_time
                        
                        metrics["cross_validation"] = {
                            "scores": scores.tolist(),
                            "mean": float(scores.mean()),
                            "std": float(scores.std()),
                        }
                        logger.info(f"êµì°¨ ê²€ì¦ ì™„ë£Œ ({cv_duration:.2f}ì´ˆ)")
                        logger.info(f"CV {scoring}: {scores.mean():.4f} (Â±{scores.std():.4f})")

                    # MLflow ë©”íŠ¸ë¦­ ë¡œê¹…
                    logger.info("ë©”íŠ¸ë¦­ ë¡œê¹… ì¤‘...")
                    metric_count = 0
                    for mtype, d in metrics.items():
                        for k, v in d.items():
                            if isinstance(v, (int, float)):
                                mlflow.log_metric(f"{mtype}_{k}", float(v))
                                metric_count += 1
                    logger.info(f"ë©”íŠ¸ë¦­ ë¡œê¹… ì™„ë£Œ ({metric_count}ê°œ)")

                    # ëª¨ë¸ ì €ì¥ (ê°œì„ ëœ ë²„ì „)
                    logger.info("ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì¤‘...")
                    local_pkl = f"/tmp/{run_id}.pkl"
                    joblib.dump(est, local_pkl)
                    
                    model_saved = False
                    
                    # ë‹¨ê³„ë³„ ëª¨ë¸ ì €ì¥ ì‹œë„
                    try:
                        from mlflow.models.signature import infer_signature
                        signature = infer_signature(X_train, y_pred_test)
                        input_example = X_train.iloc[:3] if hasattr(X_train, 'iloc') else X_train[:3]
                        
                        mlflow.sklearn.log_model(
                            est, 
                            artifact_path="model",
                            signature=signature,
                            input_example=input_example,
                            registered_model_name=None,
                            await_registration_for=0
                        )
                        logger.info("âœ… ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì™„ë£Œ (signature + input_example)")
                        model_saved = True
                        
                    except Exception as signature_error:
                        logger.warning(f"âš ï¸ Signature í¬í•¨ ì €ì¥ ì‹¤íŒ¨: {signature_error}")
                        
                        try:
                            mlflow.sklearn.log_model(
                                est, 
                                artifact_path="model",
                                registered_model_name=None,
                                await_registration_for=0
                            )
                            logger.info("âœ… ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì™„ë£Œ (ê¸°ë³¸)")
                            model_saved = True
                            
                        except Exception as basic_error:
                            logger.warning(f"âš ï¸ ê¸°ë³¸ ëª¨ë¸ ì €ì¥ë„ ì‹¤íŒ¨: {basic_error}")
                            
                            try:
                                mlflow.log_artifact(local_pkl, artifact_path="model_files")
                                logger.info("âœ… ëª¨ë¸ íŒŒì¼ ìˆ˜ë™ ì—…ë¡œë“œ ì™„ë£Œ")
                                model_saved = True
                                
                            except Exception as manual_error:
                                logger.error(f"âŒ ëª¨ë“  ëª¨ë¸ ì €ì¥ ë°©ë²• ì‹¤íŒ¨: {manual_error}")
                    
                    finally:
                        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                        if os.path.exists(local_pkl):
                            os.unlink(local_pkl)

                    if not model_saved:
                        logger.warning("âš ï¸ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì‹¤íŒ¨")

                    summary = {
                        "run_id": run_id,
                        "algorithm": name,
                        "metrics": metrics,
                        "training_duration": time.time() - model_start_time,
                        "model_saved": model_saved,
                        "hpo_used": use_hpo and bool(hpo_results),
                        "hpo_results": hpo_results,
                        "final_params": model_params,
                        "task": task,
                        "execution_id": execution_id
                    }
                    results.append(summary)

                    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì—…ë°ì´íŠ¸
                    score = metrics["test"][best_key]
                    if score > best_score:
                        best_score = score
                        best = summary
                        hpo_info = f" (HPO)" if summary.get("hpo_used") else ""
                        logger.info(f"ğŸ† ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ ëª¨ë¸: {name}{hpo_info} ({best_key}={score:.4f})")
                    
                    model_duration = time.time() - model_start_time
                    logger.info(f"âœ… {name} ëª¨ë¸ ì™„ë£Œ ({model_duration:.2f}ì´ˆ)")
                    
            except Exception as e:
                logger.error(f"âŒ {name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                logger.error(f"ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                continue

        # ì•ˆì „ë§: bestê°€ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨
        if not best:
            error_msg = "ëª¨ë“  ëª¨ë¸ í•™ìŠµì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
            logger.error(f"âŒ {error_msg}")
            raise RuntimeError(error_msg)

        hpo_info = f" (HPO)" if best.get("hpo_used") else ""
        logger.info(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best['algorithm']}{hpo_info} (Run ID: {best['run_id']})")
        logger.info(f"ìµœê³  {best_key}: {best_score:.4f}")
        
        if best.get("hpo_used") and best.get("hpo_results"):
            logger.info(f"HPO ìµœì  íŒŒë¼ë¯¸í„°: {best['final_params']}")

        # manifest ìƒì„± (HPO ë° ë¼ë²¨ ì¸ì½”ë”© ì •ë³´ í¬í•¨)
        logger.info("\nğŸ“„ Manifest ìƒì„± ì¤‘...")
        manifest = {
            "results": results,
            "best": best,
            "feature_names": used_feature_names,
            "model_id": model_id,
            "execution_id": execution_id,
            "task": task,
            "training_timestamp": datetime.now().isoformat(),
            "training_duration": time.time() - training_start_time,
            "data_info": {
                "hf_repo": hf_repo,
                "hf_filename": hf_filename,
                "hf_revision": hf_revision or "default",
                "target_column": target_column,
                "feature_count": len(used_feature_names),
                "data_shape": df.shape
            },
            "hpo_summary": {
                "enabled": use_hpo,
                "models_optimized": sum(1 for r in results if r.get("hpo_used", False)) if use_hpo else 0,
                "config": hpo_config if use_hpo else None
            },
            # ë¼ë²¨ ì¸ì½”ë”© ì •ë³´ ì¶”ê°€
            "label_encoding": {
                "used": label_encoder is not None,
                "original_classes": original_classes,
                "label_mapping": label_mapping
            } if task == "classification" else None,
            # ë²„ì „ ê´€ë¦¬ ì •ë³´ ì¶”ê°€
            "version_info": {
                "unique_execution": USE_UNIQUE_PATHS,
                "cleanup_enabled": CLEANUP_TEMP_FILES,
                "temp_directories": temp_directories if temp_directories else []
            }
        }

        # ë² ìŠ¤íŠ¸ run ì•„í‹°íŒ©íŠ¸ë¡œ manifest.json ì €ì¥
        logger.info("ë² ìŠ¤íŠ¸ runì— manifest ì €ì¥ ì¤‘...")
        tmp_manifest = None
        try:
            client = MlflowClient()
            tmp_manifest = tempfile.NamedTemporaryFile(delete=False, suffix=".json").name
            with open(tmp_manifest, "w", encoding="utf-8") as f:
                json.dump(manifest, f, ensure_ascii=False, indent=2)
            client.log_artifact(best["run_id"], tmp_manifest, artifact_path="manifest")
            logger.info("âœ… Manifest ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âš ï¸  Manifest ì €ì¥ ì‹¤íŒ¨: {e}")
        finally:
            if tmp_manifest and os.path.exists(tmp_manifest):
                os.unlink(tmp_manifest)

        # ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ì—ë„ ì €ì¥ (ì„ íƒì‚¬í•­)
        runs_manifest_uri: Optional[str] = None
        if artifact_base_uri:
            logger.info("ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ì— manifest ì—…ë¡œë“œ ì¤‘...")
            try:
                storage = StorageClient(
                    artifact_base_uri,
                    **(storage_ctor_kwargs or {}),
                )
                
                # ê³ ìœ í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ì¶©ëŒ ë°©ì§€)
                if USE_UNIQUE_PATHS:
                    storage_key = f"models/{model_id}/{execution_id}_runs.json"
                else:
                    storage_key = f"models/{model_id}/_last_runs.json"
                
                tmp_manifest_storage = tempfile.NamedTemporaryFile(delete=False, suffix=".json").name
                with open(tmp_manifest_storage, "w", encoding="utf-8") as f:
                    json.dump(manifest, f, ensure_ascii=False, indent=2)
                
                runs_manifest_uri = storage.upload_file(tmp_manifest_storage, storage_key)
                os.unlink(tmp_manifest_storage)
                logger.info(f"âœ… ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ì™„ë£Œ: {runs_manifest_uri}")
            except Exception as e:
                logger.error(f"âš ï¸  ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        # MLflow Model Registry ë“±ë¡ & Production ìŠ¹ê²©
        production_version = None
        if os.getenv("ENABLE_MODEL_REGISTRY", "true").lower() == "true" and best.get("model_saved", False):
            try:
                # ë²„ì „ íƒœê·¸ ìƒì„±
                version_tags = {
                    "execution_id": execution_id,
                    "training_timestamp": datetime.now().isoformat(),
                    "data_source": f"{hf_repo}/{hf_filename}",
                    "unique_run": str(USE_UNIQUE_PATHS),
                }
                
                if best.get("hpo_used"):
                    version_tags["hpo_optimized"] = "true"
                    version_tags["hpo_trials"] = str(best.get("hpo_results", {}).get("n_trials", 0))
                
                if label_encoder is not None:
                    version_tags["label_encoded"] = "true"
                    version_tags["num_classes"] = str(len(original_classes))
                
                production_version = _register_best_to_mlflow(
                    model_name=model_id,
                    best=best,
                    feature_names=used_feature_names,
                    version_tags=version_tags
                )
            except Exception as e:
                logger.error(f"âš ï¸  Model Registry ë“±ë¡ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
                production_version = None
        elif not best.get("model_saved", False):
            logger.warning("âš ï¸  ë² ìŠ¤íŠ¸ ëª¨ë¸ì˜ ì•„í‹°íŒ©íŠ¸ê°€ ì €ì¥ë˜ì§€ ì•Šì•„ Model Registry ë“±ë¡ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        else:
            logger.info("â„¹ï¸  Model Registry ë¹„í™œì„±í™”ë¨ (ENABLE_MODEL_REGISTRY=false)")

        total_duration = time.time() - training_start_time
        logger.info(f"\nğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ! ({total_duration:.2f}ì´ˆ)")
        logger.info(f"ì‹¤í–‰ ID: {execution_id}")
        logger.info(f"í•™ìŠµëœ ëª¨ë¸ ìˆ˜: {len(results)}")
        logger.info(f"ë² ìŠ¤íŠ¸ ëª¨ë¸: {best['algorithm']}")
        if use_hpo:
            hpo_count = sum(1 for r in results if r.get("hpo_used", False))
            logger.info(f"HPO ì ìš©ëœ ëª¨ë¸ ìˆ˜: {hpo_count}/{len(results)}")
        if label_encoder is not None:
            logger.info(f"ë¼ë²¨ ì¸ì½”ë”© ì ìš©: {original_classes} -> {list(range(len(original_classes)))}")
        if production_version:
            logger.info(f"Model Registry: {model_id} v{production_version} (Production)")
        logger.info("=" * 80)

        # ë°˜í™˜ ë°ì´í„°ì— ëª¨ë“  ì •ë³´ í¬í•¨
        return {
            "results": results,
            "best": best,
            "execution_id": execution_id,
            "runs_manifest_uri": runs_manifest_uri or "",
            "registry": {
                "model_name": model_id, 
                "production_version": production_version or "",
                "version_tags": version_tags if 'version_tags' in locals() else {}
            },
            "training_duration": total_duration,
            "feature_names": used_feature_names,
            "training_timestamp": datetime.now().isoformat(),
            "data_info": {
                "hf_repo": hf_repo,
                "hf_filename": hf_filename,
                "hf_revision": hf_revision or "default",
                "target_column": target_column,
                "feature_count": len(used_feature_names),
                "data_shape": df.shape
            },
            "hpo_summary": {
                "enabled": use_hpo,
                "models_optimized": sum(1 for r in results if r.get("hpo_used", False)) if use_hpo else 0,
                "config": hpo_config if use_hpo else None
            },
            # ë¼ë²¨ ì¸ì½”ë”© ì •ë³´ ë°˜í™˜
            "label_encoding": {
                "used": label_encoder is not None,
                "original_classes": original_classes,
                "label_mapping": label_mapping
            } if task == "classification" else None,
            # ë²„ì „ ê´€ë¦¬ ì •ë³´
            "version_info": {
                "unique_execution": USE_UNIQUE_PATHS,
                "cleanup_enabled": CLEANUP_TEMP_FILES,
            }
        }
        
    except Exception as e:
        total_duration = time.time() - training_start_time
        logger.error(f"\nğŸ’¥ í•™ìŠµ ì‹¤íŒ¨! ({total_duration:.2f}ì´ˆ)")
        logger.error(f"ì‹¤í–‰ ID: {execution_id}")
        logger.error(f"ì—ëŸ¬: {str(e)}")
        logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        logger.error(f"ì „ì²´ ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        logger.error("=" * 80)
        raise
    
    finally:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if CLEANUP_TEMP_FILES and temp_directories:
            logger.info("ğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘...")
            import shutil
            for temp_dir in temp_directories:
                try:
                    if os.path.exists(temp_dir):
                        shutil.rmtree(temp_dir, ignore_errors=True)
                        logger.info(f"âœ… ì •ë¦¬ ì™„ë£Œ: {temp_dir}")
                except Exception as cleanup_error:
                    logger.warning(f"âš ï¸  ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {temp_dir} - {cleanup_error}")